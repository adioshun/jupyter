{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 이번에는 현재 들어온 인풋값만이 아니라 과거의 데이터와 현재의 데이터 모두를 바탕으로\n",
    "# 판단하는 Recurrent Neural Network를 만들어보겠습니다.\n",
    "# cs231n 강의에도 나오는 \"Hello\"라는 스트링에서 \"Hell\"이 들어오면 \"ello\"라는\n",
    "# 결과값을 만들어내는 RNN 코드를 짜보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 먼저 Numpy와 Tensorflow를 불러옵니다.\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'e': 0, 'l': 2, 'h': 1, 'o': 3}\n",
      "['e', 'h', 'l', 'o']\n"
     ]
    }
   ],
   "source": [
    "# 그리고 우리가 학습시키고자하는 단어 \"Hello\"를 불러와서 여기 나오는 알파벳들을 \n",
    "# dictionary 형태로 저장해 놓습니다\n",
    "# look_up_table은 나중에 인덱스로 찾기 편하게 이를 리스트로 바꾼것입니다.\n",
    "\n",
    "input_string=\"hello\"\n",
    "string_to_char=list(input_string)\n",
    "set_of_char=set(string_to_char)\n",
    "look_up_table=list(set_of_char)\n",
    "\n",
    "dict_char={ c:idx for idx,c in enumerate(set_of_char)}\n",
    "print(dict_char)\n",
    "print(look_up_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2, 2] [0, 2, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# 인풋인 hell과 아웃풋인 ello 역시 만들어줍니다.\n",
    "\n",
    "x1=[dict_char[i] for i in input_string[:-1]]\n",
    "y1=[dict_char[i] for i in input_string[1:]]\n",
    "length=len(x1)\n",
    "\n",
    "print(x1,y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 위에서는 인덱스였지만 인풋으로 넣을때는 one-hot encoding으로 바꿔줍니다\n",
    "# 이렇게 바꾸는 이유는 l이 0이고 o는 3인것이 수치적으로 의미가 없고 그냥 어쩌다보니\n",
    "# 저렇게 관계가 정해진것이기 때문입니다.\n",
    "# 인덱스가 들어왔을때 one-hot으로 바꿔주는 함수를 만듭니다.\n",
    "\n",
    "def index_to_onehot(x,length):\n",
    "\tarr=[]\n",
    "\tfor i in x:\n",
    "\t\tnew_arr=[int(i==j) for j in range(length)]\n",
    "\t\tarr.append(new_arr)\n",
    "\treturn arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  0.  0.]\n",
      " [ 1.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.]]\n",
      "[0, 2, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# 이제 인풋과 아웃풋을 지정해줍니다.\n",
    "# 인풋같은 경우는 one-hot으로 바꿔주고 output은 tf.argmax로 인덱스를 추출하면\n",
    "# 되기 때문에 그대로 전달해줍니다.\n",
    "\n",
    "input_x =np.array(index_to_onehot(x1,length),dtype=np.float32)\n",
    "output_y_ =y1\n",
    "\n",
    "print(input_x)\n",
    "print(output_y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# 이제 RNN 모델을 만들고 학습시킬때 필요한 인자들을 지정해봅시다\n",
    "# 각 인자에 대한 설명은 한칸 밑에서 하도록 하겠습니다.\n",
    "\n",
    "num_units=length\n",
    "batch_size=1\n",
    "learning_rate=1e-2\n",
    "\n",
    "print(num_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RNN셀을 쓸때 요즘에는 LSTM이 기본이지만 예제이기 때문에 기본 RNN 셀을 사용하겠습니다.\n",
    "# LSTM에 대해 궁금하신분은 여기서 보시면 됩니다 \n",
    "# http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "# 기본 RNN은 텐서플로우 홈페이지에 다음과 같이 정의되어 있습니다\n",
    "# tf.nn.rnn_cell.BasicRNNCell.__init__(num_units, input_size=None, activation=tanh) \n",
    "# 여기서 num_units는 그냥 뉴럴넷에서 히든레이어 노드 수라고 보면됩니다\n",
    "# tf.RNN에서 hidden layer == output으로 보기 때문에 지금 예시같은 경우에는 \n",
    "# h,e,l,o니까 4개 또는 length입니다\n",
    "\n",
    "rnn_cell=tf.nn.rnn_cell.BasicRNNCell(num_units=num_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'split:0' shape=(1, 4) dtype=float32>, <tf.Tensor 'split:1' shape=(1, 4) dtype=float32>, <tf.Tensor 'split:2' shape=(1, 4) dtype=float32>, <tf.Tensor 'split:3' shape=(1, 4) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "# 또한 RNN은 t=0을 시작이라고 했을때 t=-1일때의 initial state가 필요하기 때문에\n",
    "# 셀의 크기에 맞춰 다 0으로 초기화 해놓습니다\n",
    "# 또한 지금 input_x는 4x4 행렬인데 이를 1x4씩 시간에 따라 넣어줘야하기 때문에\n",
    "# tf.split(split_dim, num_split, value, name='split')\n",
    "# split_dim 기준으로 num_split 등분해주는 함수입니다\n",
    "\n",
    "hidden_state_initial=rnn_cell.zero_state(batch_size,dtype=tf.float32)\n",
    "input_x_split=tf.split(0,length,input_x)\n",
    "\n",
    "print(input_x_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'RNN/BasicRNNCell/Tanh:0' shape=(1, 4) dtype=float32>, <tf.Tensor 'RNN/BasicRNNCell_1/Tanh:0' shape=(1, 4) dtype=float32>, <tf.Tensor 'RNN/BasicRNNCell_2/Tanh:0' shape=(1, 4) dtype=float32>, <tf.Tensor 'RNN/BasicRNNCell_3/Tanh:0' shape=(1, 4) dtype=float32>]\n",
      "Tensor(\"RNN/BasicRNNCell_3/Tanh:0\", shape=(1, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# RNN 모델에 관한 설명은 아래의 주소에 있습니다. 아직 api에는 없어서 찾기 힘들었습니다\n",
    "# https://github.com/ahangchen/TensorFlowDoc/blob/master/api_docs/python/functions_and_classes/shard0/tf.nn.rnn.md\n",
    "# tf.nn.rnn(cell, inputs, initial_state=None, dtype=None, sequence_length=None, scope=None)\n",
    "\n",
    "outputs, state = tf.nn.rnn(rnn_cell, input_x_split, hidden_state_initial)\n",
    "\n",
    "print(outputs)\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 이렇게 나온 output을 가지고 loss를 계산하고 backpropagate해야 하는데 \n",
    "# 그냥 numpy만 가지고 하면 상당히 복잡합니다\n",
    "# 텐서플로우는 친절하게 이러한 경우에도 자동으로 backprop되게 구현을 해놓았는데\n",
    "# 이때 사용하는 함수가 tf.nn.seq2seq.sequence_loss_by_example()입니다\n",
    "# tf.contrib.legacy_seq2seq.sequence_loss_by_example(logits, targets, weights, average_across_timesteps=True, softmax_loss_function=None, name=None) \n",
    "# https://github.com/tensorflow/tensorflow/blob/287db3a9b0701021f302e7bb58af5cf89fdcd424/tensorflow/g3doc/api_docs/python/functions_and_classes/shard5/tf.contrib.legacy_seq2seq.sequence_loss_by_example.md#tfcontriblegacy_seq2seqsequence_loss_by_examplelogits-targets-weights-average_across_timestepstrue-softmax_loss_functionnone-namenone-sequence_loss_by_example\n",
    "# 여기에 들어가서 설명을 읽어보면 logits은 [batch_size x num_decoder_symbols]\n",
    "# targets는 logits와 같은 길이의 1차원 텐서, weights는 각 time 별로의 가중치입니다\n",
    "# 상당히 복잡하기 때문에 한가지씩 설명해드리겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape:0\", shape=(4, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 먼저 logits입니다. RNN에서 돌린 결과는 각각 time별로의 output이고 예시에서는 1x4짜리 배열입니다\n",
    "# 이 결과들을 [batch x num_decoder_symbols]로 바꾸려면 일단 output들을 concatenate해야합니다\n",
    "# 그 후에는 num_decoder_symbols에 맞춰 reshape 해줍니다. \n",
    "# 여기서 batch는 time에 따라 들어오는 인풋을 의미하기 때문에 결과적으로 logit은 4x4가 됩니다\n",
    "\n",
    "logits = tf.reshape(tf.concat(1, outputs), [-1, num_units])\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape_1:0\", shape=(4,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# targets 역시 length에 맞춰 reshape 해줍니다 \n",
    "# 이 코드는 돌려도 형태에는 변화가 없습니다\n",
    "\n",
    "targets = tf.reshape(output_y_, [-1])\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ones:0\", shape=(4,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# weights는 time별 가중치인데 여기서는 모두 같게 주기 때문에\n",
    "# 편하게 tf.ones를 사용했습니다.\n",
    "\n",
    "weights = tf.ones([length* batch_size])\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 이제 tf.nn.seq2seq.sequence_loss_by_example를 사용해 loss를 계산해줍니다\n",
    "# []을 해주는 이유는 logits,targets,weights가 각각 array이기 때문에 \n",
    "# 콤마만으로는 어디서부터 어디까지가 경계인지 구분할 수 없기도 하고 \n",
    "# 함수 내부에서 크기 비교를 텐서가 아닌 파이썬 배열로 해놓았기 때문입니다. \n",
    "# loss를 통해 cost를 구하고 optimizer도 지정해 줍니다\n",
    "\n",
    "loss = tf.nn.seq2seq.sequence_loss_by_example([logits], [targets], [weights])\n",
    "cost = tf.reduce_sum(loss) / batch_size\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 0]\n",
      "[1 0 2 0]\n",
      "[1 0 2 0]\n",
      "[1 0 2 0]\n",
      "[1 0 2 0]\n",
      "[1 0 2 0]\n",
      "[1 0 2 0]\n",
      "[1 0 2 0]\n",
      "[1 0 2 0]\n",
      "[1 0 2 0]\n",
      "[1 0 2 0]\n",
      "[1 0 2 0]\n",
      "[1 0 2 3]\n",
      "[1 0 2 3]\n",
      "[0 0 3 3]\n",
      "[0 0 3 3]\n",
      "[0 0 3 3]\n",
      "[0 0 3 3]\n",
      "[0 0 3 3]\n",
      "[0 0 3 3]\n",
      "[0 0 3 3]\n",
      "[0 0 3 3]\n",
      "[0 0 3 3]\n",
      "[0 0 3 3]\n",
      "[0 0 3 3]\n",
      "[0 0 3 3]\n",
      "[0 0 3 3]\n",
      "[0 2 3 3]\n",
      "[0 2 3 3]\n",
      "[0 2 3 3]\n",
      "[0 2 3 3]\n",
      "[0 2 3 3]\n",
      "[0 2 3 3]\n",
      "[0 2 3 3]\n",
      "[0 2 3 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3]\n",
      "[0 2 2 3] ['e', 'l', 'l', 'o']\n"
     ]
    }
   ],
   "source": [
    "# 드디어 세션을 돌릴 수 있게 되었습니다.\n",
    "# 처음에는 이상한 단어를 예상하다가 점점 알맞은 단어를 예측하게 학습되었음을 볼 수 있습니다\n",
    "# 맨 위에 input_string을 바꾸면서 다른 단어에 대해서도 테스트 할 수 있습니다.\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(100):\n",
    "        sess.run(train_op)\n",
    "        result = sess.run(tf.argmax(logits, 1))\n",
    "        print(result)\n",
    "    print(result, [look_up_table[t] for t in result])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
